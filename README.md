# Recursive Language Models (RLM) - OpenRouter Implementation

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![OpenRouter](https://img.shields.io/badge/API-OpenRouter-green.svg)](https://openrouter.ai)

Implementation of **Recursive Language Models** based on the research from [alexzhang13.github.io/blog/2025/rlm/](https://alexzhang13.github.io/blog/2025/rlm/) using the OpenRouter API.

> **TL;DR**: RLMs solve the long-context problem by storing context as a variable and letting LMs interact with it programmatically. Result: 2x better accuracy at lower cost, scales to 10M+ tokens.

## üìö What are Recursive Language Models?

RLMs are an inference strategy that enables language models to handle **unbounded context lengths** by:

1. **Storing context as a variable** in a Python REPL environment (not in the prompt)
2. **Recursively decomposing** queries into sub-problems
3. **Programmatically interacting** with context through code execution
4. **Spawning recursive LM calls** to process context chunks in parallel

### Key Advantages

- ‚úÖ **No context rot** - Root LM never sees the entire context at once
- ‚úÖ **Unbounded context** - Tested with 10M+ tokens
- ‚úÖ **Cost efficient** - RLM(GPT-4o-mini) outperforms GPT-4o at lower cost
- ‚úÖ **Interpretable** - Can trace exactly how the model processes context
- ‚úÖ **Flexible strategies** - Models learn to peek, grep, partition, and summarize

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         User Query                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Root LM (depth=0)                        ‚îÇ
‚îÇ                  Only sees the query                        ‚îÇ
‚îÇ              Can execute Python code in REPL                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   REPL Environment                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  context = [huge context stored as variable]        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                                                      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  # Root LM can execute code:                        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  peek = context[:1000]                              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  filtered = [x for x in context if "keyword" in x]  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  result = recursive_lm(query, filtered)             ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              Recursive LM Calls (depth=1)                   ‚îÇ
‚îÇ         Process context chunks in sub-queries               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üöÄ Installation

```bash
# Clone or create the project
cd /Users/e.baena/CascadeProjects/rlm-test

# Install dependencies
pip install -r requirements.txt

# Set your OpenRouter API key
export OPENROUTER_API_KEY='your-key-here'
```

## üí° Usage

### Basic Example

```python
from rlm_openrouter import create_rlm

# Create RLM instance
rlm = create_rlm(
    root_model="openai/gpt-4o",
    recursive_model="openai/gpt-4o-mini",
    verbose=True
)

# Large context (can be string, list, or dict)
context = """
[Your huge context here - can be 100k+ tokens]
"""

# Query the RLM
result = rlm.completion(
    query="What are the main themes in this document?",
    context=context
)

print(f"Answer: {result['answer']}")
print(f"Iterations: {result['iterations']}")
print(f"Total LM calls: {result['total_calls']}")
```

### Advanced Configuration

```python
from rlm_openrouter import RecursiveLM, RLMConfig

config = RLMConfig(
    root_model="anthropic/claude-3.5-sonnet",
    recursive_model="openai/gpt-4o-mini",
    max_iterations=15,
    max_recursive_depth=2,  # Allow deeper recursion
    verbose=True
)

rlm = RecursiveLM(config)
```

## üß™ Test Examples

Run the included test suite:

```bash
python test_examples.py
```

### Available Tests

1. **Needle in Haystack** - Find specific information in large context
2. **Distributional Query** - OOLONG-style structured data analysis
3. **Multi-hop Reasoning** - Connect information across multiple documents
4. **Long Context Summarization** - Summarize very long transcripts
5. **Simple Example** - Basic functionality test (recommended first)

### Example Test Output

```
TEST 5: Simple Example - Context Analysis
================================================================================

[LM Call - Depth 0] Model: openai/gpt-4o-mini
[REPL] Executed:
# Peek at context
peek = context[:500]
result = peek

[LM Response - Depth 0]: I'll analyze the reviews to calculate the average...

[RESULT] Answer: Average rating: 3.6/5. Main complaints: overpriced, poor screen quality, runs hot, poor build quality, battery drains quickly.
[RESULT] Iterations: 3
[RESULT] Total LM calls: 4
```

## üéØ Emergent Strategies

The RLM learns to use various strategies automatically:

### 1. **Peeking**
```python
# Root LM peeks at context structure
peek = context[:1000]
print(peek)
```

### 2. **Grepping/Filtering**
```python
# Root LM filters for relevant information
import re
filtered = [line for line in context.split('\n') if re.search(r'user.*12345', line)]
```

### 3. **Partition & Map**
```python
# Root LM chunks context and processes in parallel
chunks = [context[i:i+5000] for i in range(0, len(context), 5000)]
results = [recursive_lm(f"Extract key info from: {chunk}", chunk) for chunk in chunks]
```

### 4. **Summarization**
```python
# Root LM summarizes sections hierarchically
section_summaries = []
for section in sections:
    summary = recursive_lm(f"Summarize: {section[:100]}...", section)
    section_summaries.append(summary)
```

## üìä Benchmark Results (from paper)

### OOLONG Benchmark (128k tokens)
- **GPT-4o**: ~30% accuracy
- **RLM(GPT-4o-mini)**: **>60% accuracy** (2x better, same cost!)

### BrowseComp-Plus (100k documents)
- **GPT-4o + BM25**: ~40% accuracy
- **RLM(GPT-4o)**: **~55% accuracy**
- **Scales to 10M+ tokens** without degradation

## üîß How It Works

1. **User provides query + context** ‚Üí `rlm.completion(query, context)`

2. **Root LM receives**:
   - The query
   - System prompt explaining REPL environment
   - Info about context (type, size, preview)

3. **Root LM decides strategy**:
   - Peek at context structure
   - Grep/filter relevant sections
   - Partition into chunks
   - Spawn recursive LM calls

4. **Execution loop** (max 10 iterations):
   - Root LM generates Python code
   - Code executes in REPL environment
   - Results fed back to Root LM
   - Repeat until `FINAL(answer)` or `FINAL_VAR(variable)`

5. **Return final answer** with full trace

## üé® Supported Context Types

```python
# String context
context = "Long text document..."

# List context (documents)
context = [
    {"title": "Doc 1", "content": "..."},
    {"title": "Doc 2", "content": "..."},
]

# Dict context (structured data)
context = {
    "users": [...],
    "transactions": [...],
    "metadata": {...}
}
```

## üîë OpenRouter Models

Recommended model combinations:

| Use Case | Root Model | Recursive Model | Cost/Performance |
|----------|------------|-----------------|------------------|
| **Best Performance** | `openai/gpt-4o` | `openai/gpt-4o-mini` | High/High |
| **Balanced** | `anthropic/claude-3.5-sonnet` | `openai/gpt-4o-mini` | Medium/High |
| **Cost Efficient** | `openai/gpt-4o-mini` | `openai/gpt-4o-mini` | Low/Medium |
| **Experimental** | `google/gemini-2.0-flash-exp` | `openai/gpt-4o-mini` | Low/High |

See [OpenRouter Models](https://openrouter.ai/models) for full list.

## ‚ö†Ô∏è Limitations

1. **Speed**: Not optimized - each recursive call is blocking
2. **No prefix caching**: Same context chunks may be processed multiple times
3. **Cost control**: No hard limits on total API cost per query
4. **Recursion depth**: Currently limited to depth=1 (can be increased)
5. **Code safety**: Limited sandboxing of REPL execution

## üîÆ Future Improvements

- [ ] Async/parallel recursive LM calls
- [ ] Prefix caching for repeated context chunks
- [ ] Cost and time budgets
- [ ] Deeper recursion (depth > 1)
- [ ] Fine-tuned models specifically for RLM strategies
- [ ] Better REPL sandboxing
- [ ] Visualization of RLM execution trace

## üìñ References

- **Original Blog Post**: [Recursive Language Models](https://alexzhang13.github.io/blog/2025/rlm/)
- **OpenRouter API**: [openrouter.ai](https://openrouter.ai)
- **OOLONG Benchmark**: Long-context reasoning benchmark
- **BrowseComp-Plus**: Multi-hop web search benchmark

## üìù Citation

If you use this implementation, please cite the original work:

```bibtex
@article{zhang2025rlm,
  title={Recursive Language Models},
  author={Zhang, Alex L. and others},
  year={2025},
  url={https://alexzhang13.github.io/blog/2025/rlm/}
}
```

## ü§ù Contributing

This is an experimental implementation. Contributions welcome:

- Better REPL sandboxing
- Async execution
- Additional test cases
- Performance optimizations
- Support for more model providers

## üìÑ License

MIT License - Feel free to use and modify!

---

**Built with ‚ù§Ô∏è for the AI research community**
