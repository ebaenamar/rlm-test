# Recursive Language Models (RLM) - Executive Summary

## ğŸ¯ What Problem Does This Solve?

**Context Rot**: When you give an LLM a very long context (100k+ tokens), its performance degrades significantly. This happens even with models that claim 200k token context windows.

**Example**: 
- GPT-4o with 128k token context: **30% accuracy**
- RLM using GPT-4o-mini: **60% accuracy** at **lower cost**

## ğŸ’¡ The Core Insight

Instead of putting all context in the prompt:
```
âŒ LLM(huge_context + query) â†’ answer
```

Store context as a variable and let the LM interact with it programmatically:
```
âœ… RLM(query) â†’ explores context via code â†’ recursive calls â†’ answer
```

## ğŸ—ï¸ How It Works (Simple Explanation)

1. **Context stored separately** - Not in the LLM's prompt, but in a Python REPL environment
2. **Root LM gets only the query** - Plus info that context exists as a variable
3. **Root LM writes Python code** - To peek, filter, chunk, or analyze context
4. **Can spawn recursive LM calls** - To process chunks in parallel
5. **Returns final answer** - After iterative exploration

## ğŸ“Š Key Results from Research Paper

### OOLONG Benchmark (128k tokens, context rot test)
```
Traditional Approaches:
â”œâ”€ GPT-4o (direct):           30% âŒ
â”œâ”€ GPT-4o-mini (direct):      20% âŒ
â”œâ”€ RAG + GPT-4o:              25% âŒ
â””â”€ ReAct + GPT-4o:            35% âŒ

Recursive Language Models:
â””â”€ RLM(GPT-4o-mini):          60% âœ… (2x better!)
```

### BrowseComp-Plus (100k documents, multi-hop reasoning)
```
Traditional Approaches:
â”œâ”€ GPT-4o (direct):           0% (context limit exceeded)
â”œâ”€ RAG + GPT-4o:             40%
â””â”€ ReAct + GPT-4o:           42%

Recursive Language Models:
â””â”€ RLM(GPT-4o):              55% âœ…
```

### Extreme Scale Test
- **10M+ tokens**: RLM shows **no performance degradation**
- Traditional LLMs: Can't even process this much context

## ğŸ¨ Emergent Strategies

RLMs automatically learn to:

1. **Peek** - Look at context structure first
   ```python
   preview = context[:1000]
   ```

2. **Grep** - Filter for relevant information
   ```python
   relevant = [x for x in context if "keyword" in x]
   ```

3. **Partition & Map** - Chunk and process in parallel
   ```python
   chunks = [context[i:i+5000] for i in range(0, len(context), 5000)]
   results = [recursive_lm(query, chunk) for chunk in chunks]
   ```

4. **Hierarchical Summarization** - Summarize sections, then combine
   ```python
   summaries = [recursive_lm("Summarize", section) for section in sections]
   ```

## ğŸ’° Cost Comparison (128k token document)

| Method | Cost/Query | Accuracy | Winner |
|--------|------------|----------|--------|
| GPT-4o (direct) | $0.32 | 30% | âŒ |
| RAG + GPT-4o | $0.03 | 25% | âŒ |
| **RLM(GPT-4o-mini)** | **$0.02** | **60%** | **âœ…** |

**RLM wins**: 2x better accuracy at 1/16th the cost!

## ğŸš€ Perfect Use Cases

### âœ… Excellent for:
- **Long conversation analysis** (Claude Code sessions, support tickets)
- **Deep research** (100+ papers, multi-document analysis)
- **Code repository understanding** (large codebases)
- **Legal document review** (contracts, compliance)
- **Financial analysis** (years of transactions)

### âš ï¸ Not ideal for:
- Simple Q&A (use regular LLM)
- Real-time apps (use RAG - faster)
- Static knowledge base (use RAG - cheaper)
- Short contexts (<32k tokens)

## ğŸ”§ Implementation with OpenRouter

```python
from rlm_openrouter import create_rlm

# Create RLM
rlm = create_rlm(
    root_model="openai/gpt-4o",
    recursive_model="openai/gpt-4o-mini"
)

# Query with huge context
result = rlm.completion(
    query="What are the main themes?",
    context=huge_document  # Can be 10M+ tokens!
)

print(result['answer'])
```

## ğŸ“ˆ Why This Matters

### Current State (2025)
- âœ… CoT reasoning (GPT-o1, Claude 3.5)
- âœ… ReAct agents (tool use)
- ğŸš€ **RLMs (next frontier)**

### Future Impact
1. **No more context limits** - Process unbounded context
2. **Better performance** - No context rot
3. **Lower costs** - Use smaller models effectively
4. **Interpretable** - See exactly how LM processes context
5. **Trainable** - Can RL-train optimal strategies

## ğŸ¯ Quick Start

```bash
# 1. Install
cd /Users/e.baena/CascadeProjects/rlm-test
pip install -r requirements.txt

# 2. Set API key
export OPENROUTER_API_KEY='your-key-here'

# 3. Run quick start
python quickstart.py

# 4. Try advanced examples
python test_examples.py
```

## ğŸ“š Files in This Project

```
rlm-test/
â”œâ”€â”€ rlm_openrouter.py      # Core RLM implementation
â”œâ”€â”€ test_examples.py        # 5 test scenarios
â”œâ”€â”€ quickstart.py           # Simple getting started
â”œâ”€â”€ README.md               # Full documentation
â”œâ”€â”€ COMPARISON.md           # vs RAG, ReAct, etc.
â”œâ”€â”€ SUMMARY.md              # This file
â””â”€â”€ requirements.txt        # Dependencies
```

## ğŸ”® What's Next?

### Immediate Improvements
- [ ] Async/parallel recursive calls (10x faster)
- [ ] Prefix caching (lower costs)
- [ ] Cost/time budgets
- [ ] Better REPL sandboxing

### Research Directions
- [ ] Fine-tune models for RLM strategies
- [ ] Deeper recursion (depth > 1)
- [ ] Multi-modal context (images, video)
- [ ] Formal verification
- [ ] Combine with RAG for hybrid approach

## ğŸ“– Learn More

- **Original Blog**: https://alexzhang13.github.io/blog/2025/rlm/
- **OpenRouter**: https://openrouter.ai
- **This Implementation**: See README.md for full details

## ğŸ“ Key Takeaway

**RLMs solve the long-context problem** by treating context as data to be programmatically explored, not text to be stuffed into a prompt.

**Result**: Better performance, lower cost, no context limits.

---

**Built for the AI research community** ğŸš€
